{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75bbb0a1",
   "metadata": {},
   "source": [
    "This post will be based in my Undergraduate thesis 'A Study on Gradient Boosting Algorithms and Hyperparameter Optimization using Optuna' \n",
    "\n",
    "- [Original GitHub repository](https://github.com/joaomh/study_boosting_optuna_USP_undergraduate_thesis)\n",
    "- [Link to my Undergraduate thesis in PT-BR](https://bdta.abcd.usp.br/item/003122385)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ad6996",
   "metadata": {},
   "source": [
    "\n",
    "# Tables of Content:\n",
    "\n",
    "**1. [Introduction](#Introduction)**\n",
    "\n",
    "**2. [Ensemble Learning](#Ensemble)**\n",
    "\n",
    "\n",
    "**4. [AdaBoost](#Ada)** \n",
    "\n",
    "**5. [GBMs](#GBM)** \n",
    "\n",
    "**6. [XGBoost vs. CatBoost vs. LightGBM](#XGBoost)** \n",
    "\n",
    "**7. [Improving Metrics and Controlling Overfitting](#Additive)** \n",
    "\n",
    "**8. [Optuna in XGBoost vs. CatBoost vs. LightGBM](#Optuna)** \n",
    "\n",
    "**9. [Shap in XGBoost vs. CatBoost vs. LightGBM](#Shap)** \n",
    "\n",
    "**10. [Perfomance Comparison](#Additive)** \n",
    "\n",
    "**11. [Bibliography](#Bibliography)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4fcfb1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The purpose of this post is to introduce the fundamentals of boosting algorithms and the main difference between XGBoost, CatBoost and LightGBM. We will give the reader some necessary keys to well understand and use related methods and be able to design adapted solutions when needed.\n",
    "\n",
    "If we look at the [2022 Kaggle Data Science & ML Survey](https://www.kaggle.com/kaggle-survey-2022), we can see that Gradient Boosting Machines (GBMs) have been widely used in recent years. They are supervised machine learning algorithms that have consistently produced excellent results across a wide range of problems and have won numerous machine learning competitions.\n",
    "\n",
    "\n",
    "![png1](./img/kaggle_state.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363ca9c",
   "metadata": {},
   "source": [
    "# Ensemble \n",
    "Many machine learning models primarily aim for high prediction accuracy using a single model, where boosting algorithms strive to enhance predictions by sequentially training a series of weak models, with each model compensating for the weaknesses of its predecessors.\n",
    "\n",
    "First of all we need to understand Ensemble Learning, it's based on the idea of combining several simpler prediction models (weak learner), training them for the same task, and producing from them a more complex grouped model (strong learner) that is the sum of its parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce4007",
   "metadata": {},
   "source": [
    "For example, when creating an ensemble model based on several decision trees, which are simple yet high-variance models (often considered 'weak learners'), we need to aggregate them to enhance their resistance to data variations. Therefore, it makes sense to train the trees separately, allowing each one to adapt to different parts of the dataset. This way, each tree gains knowledge about various data variations, collectively improving the ensemble's predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b78d3f",
   "metadata": {},
   "source": [
    "There are various ensemble learning methods, but in this text, we will primarily focus on Boosting, which is used in GBMs, but we can mention three algorithms that aims at combining weak learners:\n",
    "\n",
    "**Bagging**: It is generally done with homogeneous predictors, each one operating independently in relation to the others, in a parallel manner. The final algorithm is then constructed by aggregating the results obtained from the base models in some form of average. Random Forest is one of the most famous algorithm.\n",
    "\n",
    "**Boosting**: Generally implemented with homogeneous predictors, applied sequentially where the posterior model depends on the predecessor, and then these models are combined in the final ensemble. GBMs work like this\n",
    "\n",
    "**Stacking**: It is typically done with heterogeneous predictors, training them in parallel, and then combining their outputs by training a meta-model that generates predictions based on the predictions of the various weak models. Here we can for combine RandomForest with DecisionTree for example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb4fcac",
   "metadata": {},
   "source": [
    "![png1](./img/boosting_bagging.png) [Image from Ensemble Learning: Bagging & Boosting](https://towardsdatascience.com/ensemble-learning-bagging-boosting-3098079e5422)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d152829d",
   "metadata": {},
   "source": [
    "# AdaBoost\n",
    "AdaBoost is a specific Boosting algorithm developed for classification problems hte original AdaBoost algorithm is designed for classification problems, where the output is either −1 or 1, and the final prediction for a given instance is a weighted sum of each generated weak classifier\n",
    "\n",
    "$$\n",
    "G(x) = sign\\bigr[\\sum^M_{m=1}\\alpha_m\\cdot G_m(x)\\bigr]\n",
    "$$\n",
    "Here, the weights $\\alpha_m$\n",
    "are computed by the boosting algorithm, and the idea is to increase the influence of weak learners that are more accurate while simultaneously penalizing those that are not.\n",
    "The weakness is identified by the weak estimator error rate\n",
    "$$err_m = \\frac{\\sum_{i=1}^Nw_i\\mathbf{I}(y_i\\neq G_m(x_i))}{\\sum_{i=1}^Nw_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b99d6e",
   "metadata": {},
   "source": [
    "\n",
    "1. Initialize the observation weights $w_i = 1/N, i = 1, 2, . . . , N .$\n",
    "2. For $m=1$ to $M$:\n",
    "\n",
    "    2.1. Fit a classifier $G_m(x)$ to the training data using weights $w_i$\n",
    "\n",
    "    2.2. Compute $err_m = \\frac{\\sum_{i=1}^Nw_i\\mathbf{1}(y_i\\neq G_m(x_i))}{\\sum_{i=1}^Nw_i}$\n",
    "\n",
    "    2.3. Compute $\\alpha_m = log((1-err_m)/err_m)$\n",
    "\n",
    "    2.4. Set $w_i \\rightarrow w_i\\cdot exp[\\alpha_m \\cdot \\mathbf{1}(y_i\\neq G_m(x_i))],i=1,2,...,N$\n",
    "\n",
    "3. Output $G(x) = sign\\bigr[\\sum^M_{m=1}\\alpha_m\\cdot G_m(x)\\bigr]$\n",
    "\n",
    "From [1][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c72016e",
   "metadata": {},
   "source": [
    "![png](img/ada.png) [Marsh, Brendan (2016). Multivariate Analysis of the Vector Boson Fusion Higgs Boson](https://www.researchgate.net/publication/306054843_Multivariate_Analysis_of_the_Vector_Boson_Fusion_Higgs_Boson)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff28eac",
   "metadata": {},
   "source": [
    "Scikit-Learn have a implementation of AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a061fb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.983"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                           n_informative=2, n_redundant=0,\n",
    "                           random_state=0, shuffle=False)\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)\n",
    "clf.predict([[0, 0, 0, 0]])\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13ab9139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an AdaBoostClassifier with a base DecisionTreeClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b10f8a",
   "metadata": {},
   "source": [
    "# GBMs\n",
    "The Gradient Boosting Machines algorithm works by optimizing any given differentiable loss function, using gradient descent [3].\n",
    "\n",
    "We can write de GBM model as \n",
    "\n",
    "$$F_M(x) = F_0(x) + \\sum_{m=1}^MF_m(x)$$\n",
    "$ \\beta_mh(x; a_m)$ are the base functions learners, where $\\beta_m$ is the weight, and $a_m$ the parameters of the learner $h$. And we have a loss function $L(y_i,F_m(x_i))$, so we would like to find all optimal values of this parameters that would minimize this loss funciton.\n",
    "$$    \\{\\beta_m,\\alpha_m\\}_1^M = {\\arg\\min}_{\\{\\beta'_m,\\alpha'_m\\}_1^M}\\sum_{i=1}^n L\\Biggl(y^{(i)},\\sum_{m=1}^M\\beta'_mh(\\mathbf{x}^{(i)};\\alpha'_m)\\Biggl)$$\n",
    "\n",
    "In this situations where is infeasible we can try a 'greedy-stagewise' approach for $m=1,2,3,...,M$\n",
    "\n",
    "$$(\\beta_m,\\alpha_m) = {\\arg\\min}_{\\beta,\\alpha}\\sum_{i=1}^n L\\Biggl(y^{(i)},F_{m-1}\\mathbf{x}^{(i)} + \\beta h(\\mathbf{x}^{(i)};\\alpha)\\Biggl)$$\n",
    "And then we can use a vectorized notation and make similar to the gradient descent formula. The learning rate, $\\eta$ shrinks the influence of the new learner.\n",
    "$$F_m(\\mathbf{X}) = F_{m-1}(\\mathbf{X}) + \\eta \\Delta_m(X)$$\n",
    "\n",
    "\n",
    "The gradient of the loss function $L$ with relation to the last estimate $F_{m−1}(x)$ is,\n",
    "$$-g_m(\\mathbf{x}^{(i)}) = -\\Bigg[\\frac{\\partial L(y^{(i)},c^{(i)})}{\\partial F(\\mathbf{x}^{(i)})}\\Bigg]$$\n",
    "\n",
    "\n",
    "Gradient of the loss function $L$ with respect to the last prediction is sometimes called pseudo-residual, and written as $r_{m−1}$ can be written as\n",
    "$$\\mathbf{r}_{m_1} = \\nabla F_{m-1}(\\mathbf{X})L(y,F_{m-1}(\\mathbf{X})) = \\nabla \\hat{y}_{m-1}L(y,\\hat{y}_{\\mathbf{m-1}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a79938",
   "metadata": {},
   "source": [
    "\n",
    "1. $F_0(\\mathbf{X} = \\arg\\min_v\\sum_{i=1}^n L(y^{(i)},v)$\n",
    "2. For $m=1$ to $M$:\n",
    "\n",
    "    2.1. $\\mathbf{r}_{m_1} = \\nabla \\hat{y}_{m-1}L(y,\\hat{y}_{\\mathbf{m-1}})$ # Train a base learner minimizing squared error\n",
    "\n",
    "    2.2. $\\alpha = {\\arg\\min}_{\\alpha,\\beta}\\sum_{i=1}^n(\\mathbf{r}_{m-1}^{(i)}-\\beta h(\\mathbf{x}^{(i)};\\alpha))^2$\n",
    "\n",
    "    2.3. $\\beta = {\\arg\\min}_{\\beta}\\sum_{i=1}^nL(y^{(i)},F_{m-1}(\\mathbf{x}^{(i))}+\\beta h(\\mathbf{x}^{(i))};\\alpha_m)$\n",
    "\n",
    "    2.4. $\\Delta_m(X) = \\beta_mh(\\mathbf{X};\\alpha_m)$\n",
    "    \n",
    "    2.5 $F_m(\\mathbf{X}) = F_{m-1}(\\mathbf{X}) + \\eta \\Delta_m(X)$                                                              \n",
    "\n",
    "3. Output $F_m$\n",
    "\n",
    "From [3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa07ae0c",
   "metadata": {},
   "source": [
    "We also can find Gradient Boosting function in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57e67d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model = GradientBoostingClassifier()\n",
    "model.fit(X,y)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743955d7",
   "metadata": {},
   "source": [
    "# XGBoost vs. CatBoost vs. LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8f272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb18a5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89413ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd84de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043e0b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "642dabe7",
   "metadata": {},
   "source": [
    "# Bibliography\n",
    "You can find all files in [this repository](https://github.com/joaomh/xgboost-catboost-lgbm)\n",
    "\n",
    "Some references\n",
    "\n",
    "\n",
    "[1]-[Schapire, Robert E(1999). A Short Introduction to Boosting](https://cseweb.ucsd.edu/~yfreund/papers/IntroToBoosting.pdf)\n",
    "\n",
    "[2]-[HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J. (2009). The Elements of Statistical Learning](https://hastie.su.domains/Papers/ESLII.pdf)\n",
    "\n",
    "[3]-[Jerome H. Friedman (2001). GREEDY FUNCTION APPROXIMATION:\n",
    "A GRADIENT BOOSTING MACHINE](https://jerryfriedman.su.domains/ftp/trebst.pdf)\n",
    "\n",
    "- [Pinheiro, J., & Becker, M.. (2023). Um estudo sobre algoritmos de Boosting e a otimização de hiperparâmetros utilizando optuna.](https://bdta.abcd.usp.br/item/003122385)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
